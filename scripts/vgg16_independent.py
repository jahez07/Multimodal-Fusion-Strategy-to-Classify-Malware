# Import necessary packages
import os
import glob
import gc

import cv2
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import tensorflow as tf

sh_data = os.path.join("/Entropy")

folders = ['Lollipop', 'Kelihos_ver3', 'Gatak', 'Kelihos_ver1', 'Obfuscator.ACY', 'Simda', 'Tracur', 'Vundo', 'Ramnit']

# capturing training data and labels into respective lists
images = []
sh_image_labels = []

for folder in folders:
    sub_folder = os.path.join(sh_data, folder)
    print(sub_folder)
    for file_ in os.listdir(sub_folder):
        img_path = os.path.join(sub_folder, file_)
        with Image.open(img_path) as img:
            img = img.resize((224,224))
            img_array = np.array(img)
            images.append(img_array)
            sh_image_labels.append(folder)

# convert the list into arrays
sh_images = np.array(images)
sh_image_labels = np.array(sh_image_labels)

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(sh_images, sh_image_labels, test_size=0.2, random_state=42)

print("X_train shape:",x_train.shape)
print("y_train shape:",y_train.shape)
print("X_test shape:",x_test.shape)
print("y_test shape:",y_test.shape)

#Encode labels from text to integers.
from sklearn import preprocessing

le = preprocessing.LabelEncoder()

le.fit(y_train)
y_train_encoded = le.transform(y_train) #train labels

le.fit(y_test)
y_test_encoded = le.transform(y_test) #test labels

# One-hot encode the labels
y_train_encoded = tf.keras.utils.to_categorical(y_train_encoded, num_classes=9)
y_test_encoded = tf.keras.utils.to_categorical(y_test_encoded, num_classes = 9)

# Commented out IPython magic to ensure Python compatibility.
# %pip install keras_tuner

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Conv1D
from tensorflow.keras import layers
from keras_tuner.tuners import RandomSearch
from tensorflow import keras
from tensorflow.keras.applications import VGG16


def build_model(hp):

    model_1 = Sequential()

    vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    # Freeze the weights of all layers in the VGG16 model
    for layer in vgg.layers:
        layer.trainable = False

    # Add the VGG16 model to your own model
    model_1.add(vgg)

    # Add a 1D convolutional layer
    model_1.add(Conv2D(filters=32, kernel_size=3, activation='relu'))  # Example parameters, you can tune these

    # Remove the Flatten layer to maintain the spatial structure
    # model_1.add(Flatten())

    # Add the dense layer
    model_1.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=512, step=32),
                    activation='relu'))

    model_1.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))

    # adding batch normalization layer
    model_1.add(keras.layers.BatchNormalization())

    model_1.add(Dense(units = hp.Int('extra_dense_units', min_value = 32, max_value = 512, step = 32), activation = 'relu'))

    # Add another Conv1D layer before the output layer
    model_1.add(Conv2D(filters=64, kernel_size=3, activation='relu'))

    # Flatten the output before the final dense layer
    model_1.add(Flatten())

    # Add the output layer
    model_1.add(Dense(units=9, activation='softmax'))

    # Compile the model
    model_1.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model_1

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=3,
    executions_per_trial=3,
    overwrite = True,
    directory='/content/drive/MyDrive/Jahez_Vinod_2023/Big2015/entropyGraph/Tuned_Model',
    project_name='big_EG_img_vgg5'
)

tuner.search(x_train, y_train_encoded,
             epochs=10,
             validation_data=(x_test, y_test_encoded))

# best_model
best_model = tuner.get_best_models(num_models=1)[0]

# best_hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]

print(best_hyperparameters)

tf.config.run_functions_eagerly(True)

# re-tranining the model
history = best_model.fit(x_train, y_train_encoded, epochs=10,validation_data=(x_test, y_test_encoded),initial_epoch=3)

result = best_model.evaluate(x_test,y_test_encoded)

# model loss and accuracy
print("model loss:",result[0])
print("model accuracy",result[1])

# saving the cnn model
best_model.save('/content/drive/MyDrive/Jahez_Vinod_2023/Big2015/entropyGraph/Models/Model[NEW]big_Entropy_img_vgg.h5')

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'b', label='Training accurarcy')
plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
plt.title('Training and Validation accurarcy')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.figure()

# Train and validation loss
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and Validation loss')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

y_predict = []
y_actual = []
for i in range(0,len(x_test)):
    y_actual.append(np.argmax(y_test[i]))
    y_predict.append(np.argmax(best_model.predict(x_test[i].reshape(1,224,224,3))))

import sklearn
print(sklearn.__version__)

from sklearn.metrics import confusion_matrix

label = np.arange(9)
matrix = confusion_matrix(y_predict,y_actual,normalize = 'true')
print(matrix)

# Visualizing confusion matrix
import seaborn as sns

plt.figure(figsize=(40,30))
sns.set(font_scale=1.8)
fx=sns.heatmap(matrix, annot=True,fmt='.4f',cmap="GnBu")
fx.set_title('Confusion Matrix \n');
fx.set_xlabel('\n Predicted Values\n')
fx.set_ylabel('Actual Values\n');
fx.xaxis.set_ticklabels(folders)
fx.yaxis.set_ticklabels(folders)

file_path = "/content/drive/MyDrive/Jahez_Vinod_2023/Big2015/SimHash/ConfusionMatrix/"
file_name = "[NEW4GAN]Confusion_Matrix_Entropy.png"
plt.savefig(file_path + file_name)

plt.show()

# classficaton report
from sklearn.metrics import classification_report
import pandas as pd

report_dict = classification_report(y_actual,y_predict,target_names=folders, output_dict=True)

# Convert the dictionary to a Pandas DataFrame
report_df = pd.DataFrame.from_dict(report_dict).transpose()

total_support = report_df['support'].sum()
report_df.loc['accuracy', 'support'] = report_df.loc['Lollipop', 'support'] + report_df.loc['Kelihos_ver3', 'support'] + report_df.loc['Gatak', 'support'] +report_df.loc['Kelihos_ver1', 'support'] + report_df.loc['Obfuscator.ACY', 'support'] + report_df.loc['Simda', 'support'] + report_df.loc['Tracur', 'support'] + report_df.loc['Vundo', 'support'] + report_df.loc['Ramnit', 'support']

#Set the path and filename for the CSV file
file_path = "/content/drive/MyDrive/Jahez_Vinod_2023/Big2015/SimHash/Classification Report/"
file_name = "[NEW4GAN]classification_report_SimHash_vgg.csv"

# Save the DataFrame to a CSV file in Google Drive
report_df.to_csv(file_path + file_name)



from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,confusion_matrix

accuracy = accuracy_score(y_actual, y_predict)
print(f'Accuracy: {accuracy:.4f}')

# Calculate precision, recall, and F1-score
precision = precision_score(y_actual, y_predict, average='weighted')
recall = recall_score(y_actual, y_predict, average='weighted')
f1 = f1_score(y_actual, y_predict, average='weighted')
macro_f1 = f1_score(y_actual, y_predict, average='macro')

print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1-score: {f1:.4f}')
print(f'Macro F1-score: {macro_f1:.4f}')

